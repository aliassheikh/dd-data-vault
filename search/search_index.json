{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"dd-data-vault \u00b6 Manages a DANS Data Vault Storage Root Purpose \u00b6 A DANS Data Vault Storage Root is an OCFL storage root that is used to store a collection of long term preservation objects. Interfaces \u00b6 Batches and Object Import Directories \u00b6 Objects versions to be stored must be placed under the inbox in a batch directory. The layout of the batch directory is as follows: batch-dir \u251c\u2500\u2500 urn:nbn:nl:ui:13-26febff0-4fd4-4ee7-8a96-b0703b96f812 \u2502 \u251c\u2500\u2500 v1 \u2502 \u2502 \u2514\u2500\u2500 <content files> \u2502 \u251c\u2500\u2500 v2 \u2502 \u2502 \u2514\u2500\u2500 <content files> \u2502 \u251c\u2500\u2500 v2.json \u2502 \u2514\u2500\u2500 v3 \u2502 \u2514\u2500\u2500 <content files> \u251c\u2500\u2500 urn:nbn:nl:ui:13-2ced2354-3a9d-44b1-a594-107b3af99789 \u2502 \u2514\u2500\u2500 v3 \u2502 \u2514\u2500\u2500 <content files> \u2514\u2500\u2500 urn:nbn:nl:ui:13-b7c0742f-a9b2-4c11-bffe-615dbe24c8a0 \u2514\u2500\u2500 v1 \u2514\u2500\u2500 <content files> batch-dir - The batch directory is the directory where the batch of objects to be imported is placed. urn:nbn:nl:ui:13-26febff0-4fd4-4ee7-8a96-b0703b96f812 - The directory name is the identifier of the object. The pattern that an identifier must match can be configured in the configuration file. v1 , v2 , v3 - The version directories contain the content of the object version. The version directories must be named v1 , v2 , v3 , etc. When updating an existing object, the first version directory must be named after the next version to be created in the OCFL object. The service can also be configured to accept timestamps as version directories. In that case, the version directories are expected to be numbers, representing the timestamp of the version in milliseconds since the epoch. This timestamp is only used for ordering the versions in the OCFL object, so any number can be used as long as it is unique for the object. This option is mainly used for testing purposes. A version directory must be accompanied by a JSON file named vN.json , where N is the version number (e.g. v2.json for version 2). This file is required for every version. It must have the following structure: json { \"version-info\": { \"user\": { \"name\": \"John Doe\", \"email\": \"john.doe@mail.com\" }, \"message\": \"Commit message\" }, \"object-version-properties\": { \"dataset-version\": \"1.2\", \"packaging-format\": \"DANS RDA BagPack/1.0.0\" } } Requirements and notes: - The version-info object is mandatory and must include user.name , user.email , and message . - version-info.user.email may be specified with or without the mailto: prefix; the service will normalize it to mailto: . - The object-version-properties object is optional and may contain any custom properties to be stored for the object version. These are written to the Object Version Properties extension. Processing \u00b6 Order of batches \u00b6 To ensure that updates for one object are processed in the correct order, the service will process all batches in the inbox in the order they were received. Otherwise, it would be possible that a later batch would overtake an earlier batch. If these two batches contain updates for the same object, this would lead to an error because the version directory would not coincide with the next expected version in the OCFL object. Parallelization of object import directory processing \u00b6 Per batch the object import directory processing can be parallelized because there can be only one object import directory per object in a batch. The task that processes the object import directory ensures that the version directories are processed in the correct order. Automatic layer creation \u00b6 After processing a batch, the service will check if the maximum size of the layer has been reached. If this is the case, the service will create a new layer and start the archiving process of the old layer. Since object import directories are processed in parallel, it is not possible to do a more fine-grained check for the maximum size of the layer.","title":"Description"},{"location":"#dd-data-vault","text":"Manages a DANS Data Vault Storage Root","title":"dd-data-vault"},{"location":"#purpose","text":"A DANS Data Vault Storage Root is an OCFL storage root that is used to store a collection of long term preservation objects.","title":"Purpose"},{"location":"#interfaces","text":"","title":"Interfaces"},{"location":"#batches-and-object-import-directories","text":"Objects versions to be stored must be placed under the inbox in a batch directory. The layout of the batch directory is as follows: batch-dir \u251c\u2500\u2500 urn:nbn:nl:ui:13-26febff0-4fd4-4ee7-8a96-b0703b96f812 \u2502 \u251c\u2500\u2500 v1 \u2502 \u2502 \u2514\u2500\u2500 <content files> \u2502 \u251c\u2500\u2500 v2 \u2502 \u2502 \u2514\u2500\u2500 <content files> \u2502 \u251c\u2500\u2500 v2.json \u2502 \u2514\u2500\u2500 v3 \u2502 \u2514\u2500\u2500 <content files> \u251c\u2500\u2500 urn:nbn:nl:ui:13-2ced2354-3a9d-44b1-a594-107b3af99789 \u2502 \u2514\u2500\u2500 v3 \u2502 \u2514\u2500\u2500 <content files> \u2514\u2500\u2500 urn:nbn:nl:ui:13-b7c0742f-a9b2-4c11-bffe-615dbe24c8a0 \u2514\u2500\u2500 v1 \u2514\u2500\u2500 <content files> batch-dir - The batch directory is the directory where the batch of objects to be imported is placed. urn:nbn:nl:ui:13-26febff0-4fd4-4ee7-8a96-b0703b96f812 - The directory name is the identifier of the object. The pattern that an identifier must match can be configured in the configuration file. v1 , v2 , v3 - The version directories contain the content of the object version. The version directories must be named v1 , v2 , v3 , etc. When updating an existing object, the first version directory must be named after the next version to be created in the OCFL object. The service can also be configured to accept timestamps as version directories. In that case, the version directories are expected to be numbers, representing the timestamp of the version in milliseconds since the epoch. This timestamp is only used for ordering the versions in the OCFL object, so any number can be used as long as it is unique for the object. This option is mainly used for testing purposes. A version directory must be accompanied by a JSON file named vN.json , where N is the version number (e.g. v2.json for version 2). This file is required for every version. It must have the following structure: json { \"version-info\": { \"user\": { \"name\": \"John Doe\", \"email\": \"john.doe@mail.com\" }, \"message\": \"Commit message\" }, \"object-version-properties\": { \"dataset-version\": \"1.2\", \"packaging-format\": \"DANS RDA BagPack/1.0.0\" } } Requirements and notes: - The version-info object is mandatory and must include user.name , user.email , and message . - version-info.user.email may be specified with or without the mailto: prefix; the service will normalize it to mailto: . - The object-version-properties object is optional and may contain any custom properties to be stored for the object version. These are written to the Object Version Properties extension.","title":"Batches and Object Import Directories"},{"location":"#processing","text":"","title":"Processing"},{"location":"#order-of-batches","text":"To ensure that updates for one object are processed in the correct order, the service will process all batches in the inbox in the order they were received. Otherwise, it would be possible that a later batch would overtake an earlier batch. If these two batches contain updates for the same object, this would lead to an error because the version directory would not coincide with the next expected version in the OCFL object.","title":"Order of batches"},{"location":"#parallelization-of-object-import-directory-processing","text":"Per batch the object import directory processing can be parallelized because there can be only one object import directory per object in a batch. The task that processes the object import directory ensures that the version directories are processed in the correct order.","title":"Parallelization of object import directory processing"},{"location":"#automatic-layer-creation","text":"After processing a batch, the service will check if the maximum size of the layer has been reached. If this is the case, the service will create a new layer and start the archiving process of the old layer. Since object import directories are processed in parallel, it is not possible to do a more fine-grained check for the maximum size of the layer.","title":"Automatic layer creation"},{"location":"config/","text":"Configuration \u00b6 This module can be configured by editing the configuration file. This file is installed in /etc/opt/dans.knaw.nl/dd-data-vault/config.yml when using the RPM. The settings are explained with comments in the file itself. An on-line version of the latest configuration file can be found here .","title":"Configuration"},{"location":"config/#configuration","text":"This module can be configured by editing the configuration file. This file is installed in /etc/opt/dans.knaw.nl/dd-data-vault/config.yml when using the RPM. The settings are explained with comments in the file itself. An on-line version of the latest configuration file can be found here .","title":"Configuration"},{"location":"context/","text":"Context \u00b6 This module is a component in the DANS Data Station Architecture .","title":"Context"},{"location":"context/#context","text":"This module is a component in the DANS Data Station Architecture .","title":"Context"},{"location":"dev/","text":"Development \u00b6 General information about developing DANS modules can be found here . Local testing \u00b6 Local testing uses the same set-up as other DANS microservices.","title":"Overview"},{"location":"dev/#development","text":"General information about developing DANS modules can be found here .","title":"Development"},{"location":"dev/#local-testing","text":"Local testing uses the same set-up as other DANS microservices.","title":"Local testing"},{"location":"installation/","text":"Installation \u00b6 Currently, this project is built as an RPM package for RHEL8 and later. The RPM will install the binaries to /opt/dans.knaw.nl/dd-data-vault and the configuration files to /etc/opt/dans.knaw.nl/dd-data-vault . For installation on systems that do no support RPM and/or systemd: Build the tarball (see next section). Extract it to some location on your system, for example /opt/dans.knaw.nl/dd-data-vault . Start the service with the following command /opt/dans.knaw.nl/dd-data-vault/bin/dd-data-vault server /opt/dans.knaw.nl/dd-data-vault/cfg/config.yml Building from source \u00b6 Prerequisites: Java 17 or higher Maven 3.3.3 or higher RPM (optional, only if you want to build the RPM package) Steps: git clone https://github.com/DANS-KNAW/dd-data-vault.git cd dd-data-vault mvn clean install If the rpm executable is found at /usr/local/bin/rpm , the build profile that includes the RPM packaging will be activated. If rpm is available, but at a different path, then activate it by using Maven's -P switch: mvn -Pprm install . Alternatively, to build the tarball execute: mvn clean install assembly:single","title":"Installation"},{"location":"installation/#installation","text":"Currently, this project is built as an RPM package for RHEL8 and later. The RPM will install the binaries to /opt/dans.knaw.nl/dd-data-vault and the configuration files to /etc/opt/dans.knaw.nl/dd-data-vault . For installation on systems that do no support RPM and/or systemd: Build the tarball (see next section). Extract it to some location on your system, for example /opt/dans.knaw.nl/dd-data-vault . Start the service with the following command /opt/dans.knaw.nl/dd-data-vault/bin/dd-data-vault server /opt/dans.knaw.nl/dd-data-vault/cfg/config.yml","title":"Installation"},{"location":"installation/#building-from-source","text":"Prerequisites: Java 17 or higher Maven 3.3.3 or higher RPM (optional, only if you want to build the RPM package) Steps: git clone https://github.com/DANS-KNAW/dd-data-vault.git cd dd-data-vault mvn clean install If the rpm executable is found at /usr/local/bin/rpm , the build profile that includes the RPM packaging will be activated. If rpm is available, but at a different path, then activate it by using Maven's -P switch: mvn -Pprm install . Alternatively, to build the tarball execute: mvn clean install assembly:single","title":"Building from source"},{"location":"test-db/","text":"Database for testing \u00b6 Although this project can use an HSQLDB database, this will only work for very trivial scenarios, due to limitations with respect to the BLOB type. It is therefore recommended to use a PostgreSQL database, possibly on a vagrant VM. Prepare the database for testing (on the vagrant VM): sudo -u postgres psql CREATE DATABASE dd_data_vault_local_test; CREATE USER dd_data_vault_local_test WITH PASSWORD 'dd_data_vault_local_test'; GRANT ALL PRIVILEGES ON DATABASE dd_data_vault_local_test TO dd_data_vault_local_test; \\q UPDATE: for dev boxes created after 2025-10-31, this database is already present. Reset storage root \u00b6 During testing, you will often want to reset the storage root to a clean state. For this you need to: Delete all the listing records from the database: DELETE FROM listing_record; Remove the staging directory. Otherwise, the service will fail at startup because it cannot find the corresponding listing records: rm -rf data/vault/staging/* Restart the service.","title":"Database for testing"},{"location":"test-db/#database-for-testing","text":"Although this project can use an HSQLDB database, this will only work for very trivial scenarios, due to limitations with respect to the BLOB type. It is therefore recommended to use a PostgreSQL database, possibly on a vagrant VM. Prepare the database for testing (on the vagrant VM): sudo -u postgres psql CREATE DATABASE dd_data_vault_local_test; CREATE USER dd_data_vault_local_test WITH PASSWORD 'dd_data_vault_local_test'; GRANT ALL PRIVILEGES ON DATABASE dd_data_vault_local_test TO dd_data_vault_local_test; \\q UPDATE: for dev boxes created after 2025-10-31, this database is already present.","title":"Database for testing"},{"location":"test-db/#reset-storage-root","text":"During testing, you will often want to reset the storage root to a clean state. For this you need to: Delete all the listing records from the database: DELETE FROM listing_record; Remove the staging directory. Otherwise, the service will fail at startup because it cannot find the corresponding listing records: rm -rf data/vault/staging/* Restart the service.","title":"Reset storage root"},{"location":"test-dmftar/","text":"Testing with dmftar \u00b6 To test scenarios in which dmftar is used, install this command in a Python virtual environment: git submodule update to get the correct version of the dmftar source code. python3 -m venv .venv to create a virtual environment. source .venv/bin/activate to activate the virtual environment. pushd modules/dmftar to change directory to the dmftar source code. pip3 install -r requirements.txt to install the dmftar dependencies. flit install to install the dmftar command. popd to return to the root of the project. (You could also use pip3 install -e . instead of flit , if you want to edit the source code, but that is not necessary). Note that your public key must be added to the SURF Data Archive account used for testing.","title":"Testing with dmftar"},{"location":"test-dmftar/#testing-with-dmftar","text":"To test scenarios in which dmftar is used, install this command in a Python virtual environment: git submodule update to get the correct version of the dmftar source code. python3 -m venv .venv to create a virtual environment. source .venv/bin/activate to activate the virtual environment. pushd modules/dmftar to change directory to the dmftar source code. pip3 install -r requirements.txt to install the dmftar dependencies. flit install to install the dmftar command. popd to return to the root of the project. (You could also use pip3 install -e . instead of flit , if you want to edit the source code, but that is not necessary). Note that your public key must be added to the SURF Data Archive account used for testing.","title":"Testing with dmftar"},{"location":"to-api/","text":"The API is defined in dd-data-vault-api . The version implemented by this service can be viewed in Swagger UI in a new tab: API .","title":"API"}]}